{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Cleaning, Reduction & Enrichment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data scientist jobs data that I scraped from glassdoor.co.uk for analysis, I will:\n",
    "\n",
    "- **Enrich** the data: by expanding/filling-in parts of the advertised job location using an Ordinance Survey API\n",
    "\n",
    "- **Clean** the data: after importing the CSV file into a pandas DataFrame, I'll remove duplicate jobs, and check and clean the data column-by-column\n",
    "\n",
    "- **Reduce** the data: by eliminating invalid jobs and transforming the data types where possible so that they take up less memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages & Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and modules\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all columns and rows will be displayed if/when you print the dataframe\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# ensure all figures will have a white background in this notebook\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}\n",
    "\n",
    "# ignore filter warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll import the jobs data CSV file, reading it in as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 727 entries, Senior Data Scientist to Researcher/Data Scientist - QMUL\n",
      "Data columns (total 17 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   salary_estimate         453 non-null    object \n",
      " 1   job_description         724 non-null    object \n",
      " 2   rating                  587 non-null    float64\n",
      " 3   company_name            727 non-null    object \n",
      " 4   location                727 non-null    object \n",
      " 5   size                    629 non-null    object \n",
      " 6   founded                 508 non-null    float64\n",
      " 7   type_of_ownership       629 non-null    object \n",
      " 8   industry                553 non-null    object \n",
      " 9   sector                  555 non-null    object \n",
      " 10  revenue                 629 non-null    object \n",
      " 11  rating_culturevalues    572 non-null    float64\n",
      " 12  rating_worklifebalance  583 non-null    float64\n",
      " 13  rating_diversity        451 non-null    float64\n",
      " 14  rating_seniormgmt       582 non-null    float64\n",
      " 15  rating_compbenefits     583 non-null    float64\n",
      " 16  rating_careerops        583 non-null    float64\n",
      "dtypes: float64(8), object(9)\n",
      "memory usage: 102.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# create the path to the scraped and checked glassdoor jobs data\n",
    "path = './data/'\n",
    "\n",
    "# provide glassdoor scrape date\n",
    "scrapedate = '14Dec2020'  # e.g. '14Dec2020'\n",
    "\n",
    "# create the absolute path to the scraped jobs data with parsed locations\n",
    "filename = os.path.join(path, f\"gdjobs_df_{scrapedate}_checked.csv\")\n",
    "\n",
    "# read the data scientist jobs data (CSV file) into a dataframe\n",
    "gdjobs = pd.read_csv(filename, index_col=0)\n",
    "\n",
    "# display dataframe info to check that it's what you expected\n",
    "gdjobs.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichment: getting the full job location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When browsing jobs on the glassdoor.co.uk, I had noticed that the level of detail in the job locations very (e.g. 'Greater Manchester' vs 'Farnborough, Hampshire, South East England, England')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alva, Scotland                                     1\n",
       "Bromley, England                                   1\n",
       "Skipton, England                                   1\n",
       "Bristol, England                                   9\n",
       "Swindon, Wiltshire, South West England, England    2\n",
       "Colchester, England                                2\n",
       "Birmingham, England                                5\n",
       "Dundee, Scotland                                   1\n",
       "Bury St Edmunds, England                           1\n",
       "Cambridgeshire                                     1\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a sample of the job locations scraped from glassdoor.co.uk\n",
    "gdjobs['location'].value_counts().sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm interested in being able to look at jobs by region or city, etc. To make this possible, I will enrich the data set using the Ordinance Survey API to parse the location given by each employer, such that all parts of the job location are recorded in the DataFrame. For this purpose I have written the function `get_locations`, which is in the `function_locationapi.py` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727/727 [01:30<00:00,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 727 entries, 0 to 726\n",
      "Data columns (total 23 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   job_title               727 non-null    object \n",
      " 1   salary_estimate         453 non-null    object \n",
      " 2   job_description         724 non-null    object \n",
      " 3   rating                  587 non-null    float64\n",
      " 4   company_name            727 non-null    object \n",
      " 5   location                727 non-null    object \n",
      " 6   size                    629 non-null    object \n",
      " 7   founded                 508 non-null    float64\n",
      " 8   type_of_ownership       629 non-null    object \n",
      " 9   industry                553 non-null    object \n",
      " 10  sector                  555 non-null    object \n",
      " 11  revenue                 629 non-null    object \n",
      " 12  rating_culturevalues    572 non-null    float64\n",
      " 13  rating_worklifebalance  583 non-null    float64\n",
      " 14  rating_diversity        451 non-null    float64\n",
      " 15  rating_seniormgmt       582 non-null    float64\n",
      " 16  rating_compbenefits     583 non-null    float64\n",
      " 17  rating_careerops        583 non-null    float64\n",
      " 18  api_citytownvilham      694 non-null    object \n",
      " 19  api_region              694 non-null    object \n",
      " 20  api_country             698 non-null    object \n",
      " 21  uk                      727 non-null    bool   \n",
      " 22  remote                  727 non-null    bool   \n",
      "dtypes: bool(2), float64(8), object(13)\n",
      "memory usage: 120.8+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from function_locationapi import get_locations\n",
    "\n",
    "gdjobs_loc = get_locations(\n",
    "    scrapedate='14Dec2020', \n",
    "    path='./data/')\n",
    "\n",
    "# display dataframe info to check that it's what you expected\n",
    "gdjobs_loc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 new columns have been added by `get_locations()`  after using the Ordinance Survey API to parse the locations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>api_citytownvilham</th>\n",
       "      <th>api_region</th>\n",
       "      <th>api_country</th>\n",
       "      <th>uk</th>\n",
       "      <th>remote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Staines, England</td>\n",
       "      <td>Staines-upon-Thames</td>\n",
       "      <td>South East</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>Greater London</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>London, England</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Stockport, England</td>\n",
       "      <td>Stockport</td>\n",
       "      <td>North West</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Derby, England</td>\n",
       "      <td>Derby</td>\n",
       "      <td>East Midlands</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>London, England</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>Newcastle upon Tyne, England</td>\n",
       "      <td>Newcastle upon Tyne</td>\n",
       "      <td>North East</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>London, England</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Cheltenham, England</td>\n",
       "      <td>Cheltenham</td>\n",
       "      <td>South West</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         location   api_citytownvilham     api_region  \\\n",
       "196              Staines, England  Staines-upon-Thames     South East   \n",
       "590                Greater London               London         London   \n",
       "512               London, England               London         London   \n",
       "275            Stockport, England            Stockport     North West   \n",
       "135                Derby, England                Derby  East Midlands   \n",
       "627               London, England               London         London   \n",
       "503  Newcastle upon Tyne, England  Newcastle upon Tyne     North East   \n",
       "181               London, England               London         London   \n",
       "198           Cheltenham, England           Cheltenham     South West   \n",
       "54                 United Kingdom                  NaN            NaN   \n",
       "\n",
       "    api_country     uk  remote  \n",
       "196     England  False   False  \n",
       "590     England  False   False  \n",
       "512     England  False   False  \n",
       "275     England  False   False  \n",
       "135     England  False   False  \n",
       "627     England  False   False  \n",
       "503     England  False   False  \n",
       "181     England  False   False  \n",
       "198     England  False   False  \n",
       "54          NaN   True   False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdjobs_loc[['location', 'api_citytownvilham', 'api_region', 'api_country', 'uk', 'remote']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning: checking and cleaning each column as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll look at the values in each column to decide and implement any necessary cleaning of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist (KTP Associate)                              1\n",
      "NGS Product Integration Scientist                           1\n",
      "Data Scientist / Operational Researcher                     2\n",
      "Senior Data Scientist - Product                             1\n",
      "Principal Applied Scientist                                 1\n",
      "Lead Data Scientist, Performance Marketing (Belfast, UK)    2\n",
      "Senior Data Scientist - Crop Modeller R&D                   1\n",
      "Commercial Data Analyst                                     1\n",
      "Customer Data Scientist                                     1\n",
      "Data Scientist - Reinforcement Learning                     1\n",
      "Data Scientist Python PhD                                   1\n",
      "Data Science Manager                                        6\n",
      "Artificial Intelligence – Data Scientist                    1\n",
      "Chemical Development Scientist                              1\n",
      "Data Science Communicator                                   1\n",
      "Data Scientist - Defence                                    1\n",
      "Analytical Scientist 1                                      1\n",
      "Data Scientist - NLP                                        2\n",
      "Data Scientist | Python | NLP | London | Contract           1\n",
      "Customer-Facing Data Scientists                             1\n",
      "Name: job_title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check what the job titles look like\n",
    "print(gdjobs_loc['job_title'].value_counts().sample(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job titles seem to have been scraped appropriately; there are no cleaning requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "£24K-£44K (Glassdoor Est.)     1\n",
      "£39K-£44K (Glassdoor Est.)     1\n",
      "£24K-£32K (Glassdoor Est.)     1\n",
      "£40K-£52K (Glassdoor Est.)    46\n",
      "£66K-£92K (Glassdoor Est.)     2\n",
      "£33K-£51K (Glassdoor Est.)     2\n",
      "£35K-£64K (Glassdoor Est.)     1\n",
      "£26K-£32K (Glassdoor Est.)     2\n",
      "£43K-£70K (Glassdoor Est.)     1\n",
      "£48K-£60K (Glassdoor Est.)     1\n",
      "Name: salary_estimate, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check what the salary estimates look like\n",
    "print(gdjobs_loc['salary_estimate'].value_counts().sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to analyse the salary estimates I will need to isolate the numbers.\n",
    "\n",
    "It looks like all scraped salary estimates:\n",
    "- are given as ranges\n",
    "- are in GBP ('£'),\n",
    "- are per annum salaries (implied by the use of 'K' to denote thousands), and \n",
    "- end with '(Glassdoor Est.)'\n",
    "\n",
    "Before I go ahead with the cleaning, I will check whether my assumptions (above) are true to spot and address any exceptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All salaries are given as a range; no exceptions to deal with\n"
     ]
    }
   ],
   "source": [
    "# check if all salaries include a '-' (hyphen) indicating a range \n",
    "if (gdjobs_loc['salary_estimate'].dropna().str.contains('-').all()):\n",
    "    print('All salaries are given as a range; no exceptions to deal with')\n",
    "else:\n",
    "    # calculate what proportion of salary estimates are not given as a range\n",
    "    ppn_range = gdjobs_loc['salary_estimate'].dropna().str.contains('-').mean().round(2)\n",
    "    print(f\"The vast majority of Glassdoor salary estimates ({(100-ppn_range)*100}%) are given as a range\")\n",
    "    print(\"Salaries given as a single value will be used for the salary estimate midpoint directly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not all salaries are in GBP:\n",
      "\n",
      "227     $86K-$142K (Glassdoor Est.)\n",
      "471    $120K-$253K (Glassdoor Est.)\n",
      "Name: salary_estimate, dtype: object\n",
      "\n",
      "Jobs with non-GBP salary estimates removed\n"
     ]
    }
   ],
   "source": [
    "# check if all salaries are in GBP (£)\n",
    "all_gbp = gdjobs_loc['salary_estimate'].dropna().str.contains('\\£').all()\n",
    "\n",
    "if not all_gbp:\n",
    "    nongbp_se = gdjobs_loc[gdjobs_loc[\"salary_estimate\"].str.contains('\\£') == False]['salary_estimate']\n",
    "    print(f\"Not all salaries are in GBP:\\n\\n{nongbp_se}\\n\")\n",
    "    gdjobs_loc = gdjobs_loc.drop(labels=nongbp_se.index, axis='index')\n",
    "    print(\"Jobs with non-GBP salary estimates removed\")\n",
    "else:\n",
    "    print(\"All salaries are in GBP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All salaries are in thousands, denoted with a 'K'\n"
     ]
    }
   ],
   "source": [
    "# check if all salaries are in thousands (K) \n",
    "all_k = gdjobs_loc['salary_estimate'].dropna().astype(str).str.contains('K').all()\n",
    "\n",
    "if not all_k:\n",
    "    nonk_se = gdjobs_loc[gdjobs_loc[\"salary_estimate\"].astype(str).str.contains('K') == False]['salary_estimate']\n",
    "    print(f\"Not all salaries are in thousands (K; indicating annual salary):\\n\\n{nonk_se}\\n\")\n",
    "    gdjobs_loc.loc[nonk_se.index, \"salary_estimate\"] = np.nan\n",
    "    print(\"These have been converted to 'np.nan'\")\n",
    "else:\n",
    "    print(\"All salaries are in thousands, denoted with a 'K'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All salary estimates end in \"(Glassdoor Est.)\"\n"
     ]
    }
   ],
   "source": [
    "# check if all salaries end in \"(Glassdoor Est.)\"\n",
    "all_gdest = gdjobs_loc['salary_estimate'].dropna().astype(str).str.contains('(Glassdoor Est.)').all()\n",
    "\n",
    "if all_gdest:\n",
    "    print('All salary estimates end in \"(Glassdoor Est.)\"')\n",
    "else:\n",
    "    nongdest_se = gdjobs_loc[gdjobs_loc[\"salary_estimate\"].str.contains('(Glassdoor Est.)') == False]['salary_estimate']\n",
    "    print(f'Not all salaries end in \"(Glassdoor Est.)\":\\n\\n{nongdest_se}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to analyse the salary estimates more easily, I will:\n",
    "- remove all instances of \"£\", \"K\" and \"(Glassdoor Est.)\", \n",
    "- split salary ranges into min and max salary, and convert these data to numerical values\n",
    "- calculate the midpoint of the salary range by taking the mean of the min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "0     54-69\n",
      "1     58-80\n",
      "2     26-27\n",
      "10    35-40\n",
      "11    26-32\n",
      "13    46-51\n",
      "14    58-77\n",
      "15    29-38\n",
      "16    61-69\n",
      "17    51-70\n",
      "Name: salary_estimate, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove \"£\" and \"K\" from the salary_estimate column\n",
    "gdjobs_loc['salary_estimate'] = gdjobs_loc['salary_estimate'].str.replace('[K£]', '')\n",
    "\n",
    "# remove \"(Glassdoor Est.)\" by splitting string on \"(\" and keeping only the first part\n",
    "gdjobs_loc[\"salary_estimate\"] = gdjobs_loc[\"salary_estimate\"].apply(\n",
    "    lambda x: x if pd.isna(x) else x.split(\" (\")[0])\n",
    "\n",
    "# check if any instances of \"£\", \"K\", or \"(Glassdoor Est.)\" remain\n",
    "print(gdjobs_loc['salary_estimate'].str.contains('\\£').any())\n",
    "print(gdjobs_loc['salary_estimate'].str.contains('K').any())\n",
    "print(gdjobs_loc['salary_estimate'].str.contains('(Glassdoor Est.)').any())\n",
    "\n",
    "# check how the values look now\n",
    "print(gdjobs_loc['salary_estimate'].dropna().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    salary_estimate salary_min salary_max  salary_mid\n",
      "633           47-67         47         67        57.0\n",
      "15            29-38         29         38        33.5\n",
      "182           61-91         61         91        76.0\n",
      "16            61-69         61         69        65.0\n",
      "526           39-56         39         56        47.5\n",
      "552           45-62         45         62        53.5\n",
      "89            34-45         34         45        39.5\n",
      "275           37-50         37         50        43.5\n",
      "378           36-47         36         47        41.5\n",
      "616           42-52         42         52        47.0\n"
     ]
    }
   ],
   "source": [
    "# extract the min, max and mid-point of the salary estimate, where a range is given\n",
    "# split string on \"-\" and take first part\n",
    "gdjobs_loc[\"salary_min\"] = gdjobs_loc[\"salary_estimate\"].apply(\n",
    "    lambda x: x if (pd.isna(x) or (\"-\" not in x)) else x.split(\"-\")[0]\n",
    ")\n",
    "\n",
    "# max; split string on \"-\" and take second part\n",
    "gdjobs_loc[\"salary_max\"] = gdjobs_loc[\"salary_estimate\"].apply(\n",
    "    lambda x: x if (pd.isna(x) or (\"-\" not in x)) else x.split(\"-\")[1]\n",
    ")\n",
    "\n",
    "# convert the min and max to numerical values and get the midpoint\n",
    "gdjobs_loc[\"salary_mid\"] = gdjobs_loc.apply(\n",
    "    lambda x: x[\"salary_estimate\"] if pd.isna(x[\"salary_estimate\"]) else np.mean(\n",
    "        pd.to_numeric([x[\"salary_min\"], x[\"salary_max\"]])\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# check for the expected output\n",
    "print(gdjobs_loc[[\n",
    "    \"salary_estimate\", \n",
    "    \"salary_min\", \n",
    "    \"salary_max\", \n",
    "    \"salary_mid\"]].dropna().sample(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the questions I am interested in, I need all job ads in my data set to have a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    722\n",
       "True       3\n",
       "Name: job_description, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for jobs that lack a description\n",
    "gdjobs_loc['job_description'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove jobs that don't have a description\n",
    "gdjobs_loc = gdjobs_loc[gdjobs_loc['job_description'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305                       infarm\\n4.1\n",
      "717                   G-Research\\n4.7\n",
      "297     British American Tobacco\\n4.0\n",
      "42                         Logic Plum\n",
      "211                   Kelkoo LTD\\n4.4\n",
      "436                        Abcam\\n4.8\n",
      "99     Amida Recruitment Limited\\n4.4\n",
      "281       Next Phase Recruitment\\n4.0\n",
      "384                    Taylorollinson\n",
      "386                    causaLens\\n4.5\n",
      "Name: company_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# check what the scraped company names look like\n",
    "print(gdjobs_loc[\"company_name\"].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a company has a Glassdoor rating, it appears with the company's name on the website. My scraping tool has captured both the company's name and rating together, separated by a new line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the company rating has already been scraped separately and recorded in it's own column (`rating`), I'll simply remove it from the `company_name` column. I'll check each job for a rating, and when there is one, the last 4 characters of the company name will be excluded to remove the rating and new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91              GlaxoSmithKline\n",
      "409                 esure Group\n",
      "445                   CitizenMe\n",
      "348                   Sartorius\n",
      "695                       Tesco\n",
      "2                   BioGrad Ltd\n",
      "342                       Ipsos\n",
      "310      Novation Solutions Ltd\n",
      "115                 AstraZeneca\n",
      "260    Public Sector Resourcing\n",
      "Name: company_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove the company rating from the company name\n",
    "gdjobs_loc[\"company_name\"] = gdjobs_loc.apply(\n",
    "    lambda x: x[\"company_name\"] if pd.isna(x[\"rating\"]) else x[\"company_name\"][:-4], axis=1\n",
    ")\n",
    "print(gdjobs_loc[\"company_name\"].sample(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 to 50 Employees          118\n",
      "10000+ Employees           157\n",
      "1001 to 5000 Employees      92\n",
      "201 to 500 Employees        54\n",
      "5001 to 10000 Employees     21\n",
      "501 to 1000 Employees       34\n",
      "51 to 200 Employees        124\n",
      "Unknown                     25\n",
      "NaN                         97\n",
      "Name: size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check what the company sizes look like\n",
    "print(gdjobs_loc[\"size\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 issues with the `size` column:\n",
    "- Each size bracket ends with the word \"Employees\", which isn't necessary: this should be removed\n",
    "- The size values are intervals/bins of number of employees, which are essentially ordered categories, but the data type of the column is pandas object (or python string), which take up more memory than categories: I'll change the data type from object to category, specifying the order, so that the data are plotted appropriately\n",
    "- Some companies have a size called \"Unknown\": I'll convert these to NaN values so that they are excluded from analyses and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55855"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check memory usage before changing to categorical data type\n",
    "gdjobs_loc[\"size\"].memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn \"Unknown\" entries to nan values and remove \" Employees\" (10 chars) from the end\n",
    "gdjobs_loc[\"size\"] = gdjobs_loc[\"size\"].apply(\n",
    "    lambda x: np.nan if (x == \"Unknown\" or pd.isna(x)) else x[:-10]\n",
    ")\n",
    "\n",
    "# create an ordered categorical data type to apply to the 'size' column\n",
    "size_cat_type = CategoricalDtype( # CategoricalDtype allows ordering\n",
    "    categories=[\n",
    "        '1 to 50',\n",
    "        '51 to 200',\n",
    "        '201 to 500',\n",
    "        '501 to 1000',\n",
    "        '1001 to 5000',\n",
    "        '5001 to 10000',\n",
    "        '10000+'\n",
    "    ],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# change the data type to size_cat_type\n",
    "gdjobs_loc[\"size\"] = gdjobs_loc[\"size\"].astype(size_cat_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7265"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check memory usage after changing to categorical data type\n",
    "gdjobs_loc[\"size\"].memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are now in the appropriate format and using much less memory, which will make the analyses run more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Founded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348    1870.0\n",
      "14        NaN\n",
      "504    1987.0\n",
      "668       NaN\n",
      "107    1993.0\n",
      "696    2017.0\n",
      "217    1991.0\n",
      "238    2017.0\n",
      "497    2015.0\n",
      "242    2007.0\n",
      "Name: founded, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check what the founded year values look like\n",
    "print(gdjobs_loc[\"founded\"].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The year in which each company was founded are in floating point number format; I'll convert these to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     2007\n",
      "517    <NA>\n",
      "223    1987\n",
      "410    <NA>\n",
      "684    2005\n",
      "592    <NA>\n",
      "425    1992\n",
      "569    2010\n",
      "327    <NA>\n",
      "266    <NA>\n",
      "Name: founded, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# change the data type of the founded column\n",
    "gdjobs_loc[\"founded\"] = gdjobs_loc[\"founded\"].astype('Int64')\n",
    "print(gdjobs_loc[\"founded\"].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "College / University               15\n",
      "Company - Private                 385\n",
      "Company - Public                  174\n",
      "Contract                            1\n",
      "Government                          8\n",
      "Hospital                            2\n",
      "Nonprofit Organization              8\n",
      "Other Organization                  2\n",
      "Private Practice / Firm             1\n",
      "Subsidiary or Business Segment     25\n",
      "Unknown                             4\n",
      "NaN                                97\n",
      "Name: type_of_ownership, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check what the type of ownership values look like\n",
    "print(gdjobs_loc[\"type_of_ownership\"].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will take the valid values found in the jobs data set scraped on 14 Dec 2020 to create a categorical data type using CategoricalDtype; this will remove np.nan values and turn anything not listed in the CategoricalDtype into np.nan values as well. Changing the data type of the `type_of_ownership` column from pandas object/python string to categorical will also reduce memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company - Private                 385\n",
       "Company - Public                  174\n",
       "NaN                               103\n",
       "Subsidiary or Business Segment     25\n",
       "College / University               15\n",
       "Government                          8\n",
       "Nonprofit Organization              8\n",
       "Hospital                            2\n",
       "Contract                            1\n",
       "Private Practice / Firm             1\n",
       "Name: type_of_ownership, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an categorical data type to apply to the 'type_of_ownership' column\n",
    "too_cat_type = CategoricalDtype( \n",
    "    categories=[\n",
    "        'Company - Private',\n",
    "        'Company - Public',\n",
    "        'Subsidiary or Business Segment',\n",
    "        'College / University',\n",
    "        'Government',\n",
    "        'Nonprofit Organization',\n",
    "        'Hospital',\n",
    "        'Contract',\n",
    "        'Private Practice / Firm',\n",
    "    ],\n",
    "    ordered=False\n",
    ")\n",
    "\n",
    "# change the data type of gdjobs_loc[\"type_of_ownership\"] to too_cat_type\n",
    "gdjobs_loc[\"type_of_ownership\"] = gdjobs_loc[\"type_of_ownership\"].astype(too_cat_type)\n",
    "gdjobs_loc[\"type_of_ownership\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Unknown' and 'Other Organization' values, not included in the categories of the `too_cat_type` CategoricalDtype, were automatically converted to NaN values, when `too_cat_type` was applied to `gdjobs[\"type_of_ownership\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accounting                                     7\n",
      "Advertising & Marketing                        7\n",
      "Aerospace & Defense                            8\n",
      "Airlines                                       1\n",
      "Architectural & Engineering Services           7\n",
      "Banks & Credit Unions                         15\n",
      "Beauty & Personal Accessories Stores           1\n",
      "Biotech & Pharmaceuticals                     68\n",
      "Brokerage Services                             3\n",
      "Cable, Internet & Telephone Providers          2\n",
      "Camping & RV Parks                             1\n",
      "Chemical Manufacturing                         1\n",
      "Colleges & Universities                       15\n",
      "Commercial Equipment Repair & Maintenance      1\n",
      "Computer Hardware & Software                  20\n",
      "Construction                                   1\n",
      "Consulting                                    33\n",
      "Consumer Products Manufacturing                4\n",
      "Department, Clothing, & Shoe Stores           10\n",
      "Drug & Health Stores                           3\n",
      "Education Training Services                    5\n",
      "Electrical & Electronic Manufacturing          1\n",
      "Energy                                         4\n",
      "Enterprise Software & Network Solutions       39\n",
      "Federal Agencies                               6\n",
      "Financial Analytics & Research                 3\n",
      "Financial Transaction Processing               9\n",
      "Food & Beverage Stores                         5\n",
      "Food Production                                1\n",
      "Grocery Stores & Supermarkets                  3\n",
      "Health Care Products Manufacturing             2\n",
      "Health Care Services & Hospitals               9\n",
      "Health Fundraising Organizations               1\n",
      "Home Furniture & Housewares Stores             1\n",
      "Hotels, Motels, & Resorts                      1\n",
      "IT Services                                   24\n",
      "Industrial Manufacturing                       3\n",
      "Insurance Agencies & Brokerages                2\n",
      "Insurance Carriers                            15\n",
      "Internet                                      43\n",
      "Investment Banking & Asset Management         10\n",
      "Lending                                       10\n",
      "Logistics & Supply Chain                       3\n",
      "Mining                                         1\n",
      "Motion Picture Production & Distribution       1\n",
      "Oil & Gas Services                             2\n",
      "Other Retail Stores                            2\n",
      "Publishing                                     4\n",
      "Real Estate                                    1\n",
      "Research & Development                        10\n",
      "Social Assistance                              1\n",
      "Sports & Recreation                            2\n",
      "Staffing & Outsourcing                       101\n",
      "TV Broadcast & Cable Networks                  7\n",
      "Talent & Modeling Agencies                     3\n",
      "Telecommunications Services                    3\n",
      "Transportation Equipment Manufacturing         1\n",
      "Utilities                                      1\n",
      "Video Games                                    1\n",
      "NaN                                          173\n",
      "Name: industry, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check what the industry values look like\n",
    "print(gdjobs_loc[\"industry\"].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll switch the data type from pandas object to 'category' to reduce the memory usage when working with this column going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if there are any 'Unknown' industries, convert these to np.nan\n",
    "# gdjobs_loc[\"industry\"] = gdjobs_loc.apply(lambda x: np.nan if (x[\"industry\"] == \"Unknown\") else x[\"industry\"], axis=1)\n",
    "\n",
    "# change the industry column's data type from object to category\n",
    "gdjobs_loc[\"industry\"] = gdjobs_loc[\"industry\"].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accounting & Legal                      7\n",
      "Aerospace & Defense                     8\n",
      "Agriculture & Forestry                  1\n",
      "Arts, Entertainment & Recreation        5\n",
      "Biotech & Pharmaceuticals              68\n",
      "Business Services                     158\n",
      "Construction, Repair & Maintenance      2\n",
      "Education                              20\n",
      "Finance                                50\n",
      "Government                              6\n",
      "Health Care                             9\n",
      "Information Technology                126\n",
      "Insurance                              17\n",
      "Manufacturing                          12\n",
      "Media                                  13\n",
      "Mining & Metals                         1\n",
      "Non-Profit                              2\n",
      "Oil, Gas, Energy & Utilities            7\n",
      "Real Estate                             1\n",
      "Retail                                 27\n",
      "Telecommunications                      5\n",
      "Transportation & Logistics              3\n",
      "Travel & Tourism                        3\n",
      "NaN                                   171\n",
      "Name: sector, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check what the sector values look like\n",
    "print(gdjobs_loc[\"sector\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll switch the data type from pandas object to 'category' to reduce the memory usage when working with this column going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if there are any 'Unknown' sectors, convert these to np.nan\n",
    "# gdjobs_loc[\"sector\"] = gdjobs_loc.apply(lambda x: np.nan if (x[\"sector\"] == \"Unknown\") else x[\"sector\"], axis=1)\n",
    "\n",
    "# change the sector column's data type from object to category\n",
    "gdjobs_loc[\"sector\"] = gdjobs_loc[\"sector\"].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown / Non-Applicable            218\n",
      "NaN                                  97\n",
      "Less than $1 million (USD)           92\n",
      "$10+ billion (USD)                   80\n",
      "$100 to $500 million (USD)           48\n",
      "$5 to $10 billion (USD)              36\n",
      "$25 to $50 million (USD)             27\n",
      "$10 to $25 million (USD)             25\n",
      "$2 to $5 billion (USD)               25\n",
      "$50 to $100 million (USD)            21\n",
      "$500 million to $1 billion (USD)     21\n",
      "$1 to $5 million (USD)               19\n",
      "$1 to $2 billion (USD)                7\n",
      "$5 to $10 million (USD)               6\n",
      "Name: revenue, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check what the revenue values look like\n",
    "print(gdjobs_loc[\"revenue\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Unknown / Non-Applicable' values need to be converted to NaN values. I'll convert the `revenue` values from string objects to CategoricalDtype with an ordered list of valid values, so everything else (in this case, 'Unknown / Non-Applicable') is changed to a NaN value. Changing the data type of the `revenue` column from pandas object/python string to CategoricalDtype will also reduce memory usage and make plotting the data more straightforward, since the categories will be correctly ordered in any plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Less than $1 million (USD)           92\n",
       "$1 to $5 million (USD)               19\n",
       "$5 to $10 million (USD)               6\n",
       "$10 to $25 million (USD)             25\n",
       "$25 to $50 million (USD)             27\n",
       "$50 to $100 million (USD)            21\n",
       "$100 to $500 million (USD)           48\n",
       "$500 million to $1 billion (USD)     21\n",
       "$1 to $2 billion (USD)                7\n",
       "$2 to $5 billion (USD)               25\n",
       "$5 to $10 billion (USD)              36\n",
       "$10+ billion (USD)                   80\n",
       "NaN                                 315\n",
       "Name: revenue, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if there are any \"Unknown / Non-Applicable\" revenues, convert these to np.nan\n",
    "gdjobs_loc[\"revenue\"] = gdjobs_loc.apply(lambda x: np.nan if (x[\"revenue\"] == \"Unknown / Non-Applicable\") else x[\"revenue\"], axis=1)\n",
    "\n",
    "revenue_cat_type = CategoricalDtype(\n",
    "    categories=[\n",
    "        'Less than $1 million (USD)',\n",
    "        '$1 to $5 million (USD)',\n",
    "        '$5 to $10 million (USD)',\n",
    "        '$10 to $25 million (USD)',\n",
    "        '$25 to $50 million (USD)',\n",
    "        '$50 to $100 million (USD)',\n",
    "        '$100 to $500 million (USD)',\n",
    "        '$500 million to $1 billion (USD)',\n",
    "        '$1 to $2 billion (USD)',\n",
    "        '$2 to $5 billion (USD)',\n",
    "        '$5 to $10 billion (USD)',\n",
    "        '$10+ billion (USD)',\n",
    "    ],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "gdjobs_loc[\"revenue\"] = gdjobs_loc[\"revenue\"].astype(revenue_cat_type)\n",
    "gdjobs_loc[\"revenue\"].value_counts(dropna=False).sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The former 'Unknown / Non-Applicable' are now correctly included with the np.nan ('NaN') values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were up to 6 different subratings for each company when the Glassdoor website was scraped (14 Dec 2020). There is a column for each in the gdjobs DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the subrating column headers\n",
    "subratings = [\n",
    "    \"rating_culturevalues\",\n",
    "    \"rating_worklifebalance\",\n",
    "    \"rating_diversity\",\n",
    "    \"rating_seniormgmt\",\n",
    "    \"rating_compbenefits\",\n",
    "    \"rating_careerops\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 722 entries, 0 to 726\n",
      "Data columns (total 6 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   rating_culturevalues    567 non-null    float64\n",
      " 1   rating_worklifebalance  578 non-null    float64\n",
      " 2   rating_diversity        446 non-null    float64\n",
      " 3   rating_seniormgmt       577 non-null    float64\n",
      " 4   rating_compbenefits     578 non-null    float64\n",
      " 5   rating_careerops        578 non-null    float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 39.5 KB\n"
     ]
    }
   ],
   "source": [
    "# get info on the subrating columns in the jobs dataset dataframe\n",
    "gdjobs_loc[subratings].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating_culturevalues</th>\n",
       "      <th>rating_worklifebalance</th>\n",
       "      <th>rating_diversity</th>\n",
       "      <th>rating_seniormgmt</th>\n",
       "      <th>rating_compbenefits</th>\n",
       "      <th>rating_careerops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.9</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating_culturevalues  rating_worklifebalance  rating_diversity  \\\n",
       "0                   4.0                     3.9               NaN   \n",
       "1                   4.7                     4.6               4.5   \n",
       "2                   NaN                     NaN               NaN   \n",
       "3                   2.9                     2.6               3.4   \n",
       "4                   3.2                     3.1               3.4   \n",
       "\n",
       "   rating_seniormgmt  rating_compbenefits  rating_careerops  \n",
       "0                3.7                  3.8               4.1  \n",
       "1                4.4                  4.4               4.3  \n",
       "2                NaN                  NaN               NaN  \n",
       "3                2.7                  3.6               3.5  \n",
       "4                2.8                  3.3               2.9  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check what the subratings values look like\n",
    "gdjobs_loc[subratings].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check that all subratings are either NaN or a value between 0 and 5 with 1 d.p.\n",
    "for i in subratings:\n",
    "    print(\n",
    "        np.all(\n",
    "            gdjobs_loc[i].apply(\n",
    "                lambda x: pd.isna(x) or ((x*10 == np.floor(x*10)) and (x<=5 and x>0))\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data type for every subrating column is float64, and the values are either numbers between 0 and 5 rounded to 1 d.p. or NaN, no cleaning is necessary in order to analyse and plot these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsed job location columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_loc_cols = [\n",
    "    'api_citytownvilham', \n",
    "    'api_region', \n",
    "    'api_country', \n",
    "    'uk', \n",
    "    'remote'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 722 entries, 0 to 726\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   api_citytownvilham  689 non-null    object\n",
      " 1   api_region          689 non-null    object\n",
      " 2   api_country         693 non-null    object\n",
      " 3   uk                  722 non-null    bool  \n",
      " 4   remote              722 non-null    bool  \n",
      "dtypes: bool(2), object(3)\n",
      "memory usage: 24.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# get info on the parsed_loc_cols columns in the jobs dataset dataframe\n",
    "gdjobs_loc[parsed_loc_cols].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London        375\n",
      "Cambridge      47\n",
      "Manchester     22\n",
      "Edinburgh      17\n",
      "Reading        13\n",
      "Name: api_citytownvilham, dtype: int64 \n",
      "\n",
      "London             386\n",
      "East of England     80\n",
      "South East          56\n",
      "North West          40\n",
      "Scotland            30\n",
      "Name: api_region, dtype: int64 \n",
      "\n",
      "England             641\n",
      "Scotland             30\n",
      "Northern Ireland     12\n",
      "Wales                10\n",
      "Name: api_country, dtype: int64 \n",
      "\n",
      "False    708\n",
      "True      14\n",
      "Name: uk, dtype: int64 \n",
      "\n",
      "False    710\n",
      "True      12\n",
      "Name: remote, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check what the values in each location column looks like\n",
    "for i in parsed_loc_cols:\n",
    "    print(gdjobs_loc[i].value_counts().head(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value counts of the columns created from parsing the scraped locations using the Ordinance Survery API look as we'd expect. However, I'll turn the `api_region` and `api_country` columns into categorical data to reduce their memory usage. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 722 entries, 0 to 726\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype   \n",
      "---  ------              --------------  -----   \n",
      " 0   api_citytownvilham  689 non-null    object  \n",
      " 1   api_region          689 non-null    category\n",
      " 2   api_country         693 non-null    category\n",
      " 3   uk                  722 non-null    bool    \n",
      " 4   remote              722 non-null    bool    \n",
      "dtypes: bool(2), category(2), object(1)\n",
      "memory usage: 14.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# change the sector api_region and api_country columns data type to category\n",
    "gdjobs_loc[\"api_region\"] = gdjobs_loc[\"api_region\"].astype(\"category\")\n",
    "gdjobs_loc[\"api_country\"] = gdjobs_loc[\"api_country\"].astype(\"category\")\n",
    "\n",
    "# get info on the parsed_loc_cols columns in the jobs dataset dataframe\n",
    "gdjobs_loc[parsed_loc_cols].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove irrelevant jobs based on job titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin our exploratory data analysis, we want to make sure we're analysing only data scientist roles. \n",
    "\n",
    "When you search glassdoor.co.uk for a \"data scientist\" roles, jobs with wide-ranging titles are returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561                                            Data Scientist\n",
      "338       Associate Laboratory Scientist - Clinical Pathology\n",
      "351                                             Data Engineer\n",
      "190          Data Scientist Degree Apprenticeship, Ware, 2021\n",
      "456                                      Head of Data Science\n",
      "559                                            Data Scientist\n",
      "186    Transport Data Scientist, real world logistic analysis\n",
      "544                                            Data Scientist\n",
      "104                Senior Data Scientist - R&D Remote Sensing\n",
      "208                       Senior Data Scientist (Forecasting)\n",
      "556                                            Data Scientist\n",
      "240                                     Senior Data Scientist\n",
      "19                 Enzymology & Protein Engineering Scientist\n",
      "95                                               Data Analyst\n",
      "267                    Senior Data Scientist - Bioinformatics\n",
      "680                                   Curious Data Scientists\n",
      "371                     Data Scientist - Energy-Tech Start-Up\n",
      "47                                             Data Scientist\n",
      "52                                             Data Scientist\n",
      "578              Cardiovascular Renal and Metabolic Scientist\n",
      "Name: job_title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# run the code below as many times as you like \n",
    "# to get an impression of the wide-ranging job titles\n",
    "\n",
    "# print 20 random job titles from the glassdoor data set\n",
    "print(gdjobs_loc[\"job_title\"].sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having scanned the job titles (above), I have identified 6 categories of job titles returned by GlassDoor when searching for a \"data scientist\":\n",
    "\n",
    "1. Data Scientist\n",
    "2. Data Analyst\n",
    "3. Data Engineer\n",
    "4. Machine Learning/AI Engineer or Specialist\n",
    "5. Researcher\n",
    "6. Intern/Apprentice\n",
    "\n",
    "While there is overlap between all of these roles, I want to focus my analysis on jobs that best match what is commonly considered to be the role of a data scientist: an inter-disciplinary role to extract knowledge and insights from many structural and unstructured data using scientific methods, processes, and algorithms (often involving data mining, machine learning and big data).\n",
    "\n",
    "I will use regular expressions to identify whether each job title in the dataset matches any of the 6 title categories identified above, and create boolean columns to record the match results for each so that they can be used as masks to filter the data. I will use the boolean masking to select and scan the descriptions of jobs in each of the title categories (listed above) to see if they reflect a distinct job role, and whether they fit the core data scientist role that I am interested in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_description(jobsdf, col=None):\n",
    "    \"\"\"Print the description of a randomly selected job\n",
    "    \n",
    "    :param jobsdf: jobs data with job descriptions\n",
    "    :type jobsdf: pandas.core.frame.DataFrame\n",
    "    :param col: label of boolean column within jobsdf if you want a sample \n",
    "    job description from a subset of jobs (default None)\n",
    "    :type col: str\n",
    "    \"\"\"\n",
    "    # produce a random integer for the sample function\n",
    "    seed = random.randint(0, 100)\n",
    "    \n",
    "    if col is not None:  # take a sample from jobs that are True in that column\n",
    "        print(f\"\\nDescription of randomly selected job with title, '\",\n",
    "        f\"{jobsdf[jobsdf[col]]['job_title'].sample(random_state=seed).iloc[0]}':\\n\")\n",
    "        print(jobsdf[jobsdf[col]]['job_description'].sample(random_state=seed).iloc[0])\n",
    "    else:  # otherwise take a sample from all jobs\n",
    "        print(f\"\\nDescription of randomly selected job with title, '\",\n",
    "        f\"{jobsdf['job_title'].sample(random_state=seed).iloc[0]}':\\n\")\n",
    "        print(jobsdf['job_description'].sample(random_state=seed).iloc[0])\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474 (66%) job titles indicate 'data scientist' role\n"
     ]
    }
   ],
   "source": [
    "# use a regular expression to identfy \"data scientist\" job titles and \n",
    "# create a boolean mask for this category\n",
    "gdjobs_loc[\"title_datascientist\"] = gdjobs_loc[\"job_title\"].str.contains(\n",
    "    r\"data scientist\", regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "# how many job titles indicate \"data scientist\" role? (\"True\" count)\n",
    "print(\n",
    "    f\"{sum(gdjobs_loc.title_datascientist)} ({round((np.mean(gdjobs_loc.title_datascientist))*100)}%)\",\n",
    "    \"job titles indicate 'data scientist' role\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149                                              Senior Data Scientist\n",
       "75     Data Scientist | Python | Tensorflow | Deep Learning | Contract\n",
       "656                                                     Data Scientist\n",
       "190                   Data Scientist Degree Apprenticeship, Ware, 2021\n",
       "659                                            Data Scientist, Digital\n",
       "601                Power, Transmission and Distribution Data Scientist\n",
       "130                                              Senior Data Scientist\n",
       "718          Research Data Scientist, Intern - Infrastructure Strategy\n",
       "421                                           Principal Data Scientist\n",
       "600                                 Data Scientist - Marketing Science\n",
       "196                                              Senior Data Scientist\n",
       "266                                            Data Scientist (Senior)\n",
       "156                                              Senior Data Scientist\n",
       "455                   Senior Data Scientist - Electrical Power Systems\n",
       "170                                       Data Scientist - Consultancy\n",
       "108                     Data Scientist/Engineer - Medical Devices Unit\n",
       "37                                      Junior Data Scientist - London\n",
       "139                       Data Scientist - Remote - £60,000 to £80,000\n",
       "136                                  Data Scientist – Causality Expert\n",
       "696                                            Data Scientist - Remote\n",
       "Name: job_title, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display a random sample of job titles that include the string, \"data scientist\"\n",
    "gdjobs_loc.loc[gdjobs_loc['title_datascientist'],'job_title'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description of randomly selected job with title, ' Senior Data Scientist':\n",
      "\n",
      "We are notonthehighstreet.\n",
      "\n",
      "We’re home to 5,000 phenomenal small creative businesses that we are proud to call our Partners. But, now more than ever, this community needs our support. So we’re doing all we can to shine a light on these dynamic entrepreneurs, waving the flag for small businesses and generally championing their socks off.\n",
      "\n",
      "On top of our brilliant Partners, products and customers (not to mention our incredible team who have been busy beavering away from home since March), the last 12 months has seen great progress with our tech platform and customer experience. We are now looking to build on this momentum to drive our business to the next level. And that’s where you come in.\n",
      "\n",
      "What we need\n",
      "\n",
      "We’re looking for a Data Scientist to join our team to help us and our Partners continue to grow by using machine learning to improve the user experience of our site and our marketing.\n",
      "\n",
      "Reporting into the Head of Data, you and the other members of the Data Science team will collaborate with multidisciplinary groups variously composed of Software Engineers, Product Managers, UX Designers and Digital Marketeers to help develop solutions to user experience challenges and marketing opportunities.\n",
      "\n",
      "Some of the Things You’ll Be Doing\n",
      "Finding relevant products for different customers is an important goal for us. You’ll help us solve this problem by making predictions about customers’ tastes based on historical purchase patterns\n",
      "Last-click marketing attribution models don’t make sense for multi-device and multi-session user journeys so we need to do better. You’ll build advanced attribution models that identify our highest ROI marketing activity so we can target our spend more efficiently.\n",
      "We need to be able to optimise our site scientifically. A/B testing is a good start, but we need you to push the envelope with more sophisticated and efficient approaches such as multi-armed bandits\n",
      "We believe developing people’s skills is critical. As a senior data scientist, you’ll be responsible for managing and developing junior members of the Data Science team\n",
      "The data science team has several stakeholders, and we need to ensure that new work is identified early and managed efficiently. You’ll collaborate with product and marketing teams to identify, design, and iteratively improve relevant workflows and processes.\n",
      "In several functions across the business, we’re reliant on tools that include some sort of machine learning component. You’ll help non-technical stakeholders make the most of those tools by making informed recommendations. You’ll also help promote ML and educate people on how to spot problems that can be solved using it\n",
      "What we need…\n",
      "Several years of commercial experience, or postgraduate experience with at least one year of commercial experience, in which you have implemented both unsupervised and supervised machine learning techniques using large data sets\n",
      "Expertise in one or more of the following areas: recommender systems, search, text mining, reinforcement learning, attribution models\n",
      "At least two years experience with the typical Python machine learning stack (numpy, SciPy, pandas, scikit-learn etc.)\n",
      "A sound understanding of the mathematical foundations of your methods (calculus, linear algebra, statistics, probability)\n",
      "Excellent SQL skills\n",
      "Experience presenting your work to, and gathering requirements from, non-technical stakeholders\n",
      "…and some other key skills & experience you’ll ideally have\n",
      "Experience working in marketplace ecommerce businesses\n",
      "Implementation of cloud-based ML solutions\n",
      "Mentoring junior team members or graduates\n",
      "Using R\n",
      "Applying deep learning techniques\n",
      "Statistical/probabilistic knowledge of hypothesis (A/B) testing\n",
      "Writing good quality code which is designed to be reusable, testable and automatically deployable\n",
      "Agile/Lean working practices\n",
      "Our perks\n",
      "\n",
      "It’s important to us that our people are well looked after, which is why we offer everyone BUPA healthcare, life insurance and a pension plan. Because having a break is important for your physical and mental wellbeing, our holiday allowance is 25 days (plus bank holidays) AND your entitlement increases an extra day for every 2 years you spend with us.\n",
      "\n",
      "The team spirit at NOTHS House is legendary, and while we all miss the in-office massages, hair cuts and the abundance of free fruit, we’ve taken to our virtual world with gusto and bring the team together regularly for socials and of course the odd Zoom quiz!\n",
      "\n",
      "We endeavour to support our people to make sure work….well, works for them. And the pretty big issue of the recent global pandemic has propelled us into working more flexibly than ever before. With the office closed until further notice (at least until government social distancing advice has changed), we’re using the lessons learnt over this period to evolve our approach to flexible working whilst ensuring we all still get together as one #teamnoths\n",
      "\n",
      "Diversity and Inclusion\n",
      "\n",
      "Everyone’s welcome at notonthehighstreet – whoever they are, wherever their background.\n",
      "\n",
      "As part of your application you’ll be asked to complete a demographic survey to help us learn more about who wants to work with us. It’s completely optional (but really appreciated) and it’s 100% anonymous. We’ll only use the information to help us figure out how to make our team even more inclusive and attract more brilliant people to join us, with the help of our cross-team Diversity Champions to drive progress.\n",
      "\n",
      "Apply to join #teamnoths\n",
      "\n",
      "We’d love to hear more about you, your experience and why you’d like to join our team.\n",
      "\n",
      "For now, our recruitment and induction processes are fully remote, taking advantage of the wonders of modern-day technology. If you are successful in your initial application we’ll explain more about next steps, as this can vary slightly from role to role. Good luck!\n"
     ]
    }
   ],
   "source": [
    "# scan job descriptions; keep running this cell until you've reviewed enough samples\n",
    "sample_description(jobsdf=gdjobs_loc, col=\"title_datascientist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having looked at many random samples of job titles that include \"data scientist\", it often seems to be included as a general title in addition to specialisms, e.g. \"Data Scientist/Engineer\" indicating a Data Engineer role, and \"Remote Data Scientist / Machine Learning Engineer\" specifying a machine learning engineer role, \"AI Ops Data Scientist\" suggesting a DevOps role specialising in AI, and \"Data Scientist / Software Developer\", which would require someone with significant software development skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 (9%) job titles indicate a 'data analyst' role\n"
     ]
    }
   ],
   "source": [
    "# use a regular expression to identfy \"data analyst\" job titles and \n",
    "# create a boolean mask for this category\n",
    "gdjobs_loc[\"title_dataanalyst\"] = gdjobs_loc[\"job_title\"].str.contains(\n",
    "    r\"analy\", regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "# how many job titles indicate \"data analyst\" role? (\"True\" count)\n",
    "print(\n",
    "    f\"{sum(gdjobs_loc.title_dataanalyst)} ({round((np.mean(gdjobs_loc.title_dataanalyst))*100)}%)\",\n",
    "    \"job titles indicate a 'data analyst' role\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55                                                     Data Analyst\n",
      "504                                Analytical Outsourcing Scientist\n",
      "128                                                    DATA ANALYST\n",
      "466                         Data Analytics apprenticeship programme\n",
      "407          Senior Data Scientist - Innovation, Advanced Analytics\n",
      "627                               Data Scientist, Product Analytics\n",
      "48         Data Scientist, Data Analyst, Data Visualisation, Python\n",
      "497                                     Risk & Control Data Analyst\n",
      "95                                                     Data Analyst\n",
      "13                    Finance Data Analyst - Growth and Forecasting\n",
      "375                                         Business Analyst (Data)\n",
      "462                             Data & Analytics Consultant (m/f/d)\n",
      "573                        Data Science Lead, Reliability Analytics\n",
      "328              Scientific Data Analyst - Machine Learning, Python\n",
      "429                          eCommerce Data Analyst - Book Retailer\n",
      "325                                   Data Analyst, Financial Crime\n",
      "102                                   Senior Data Analyst/Scientist\n",
      "674                             Data Scientist – Customer Analytics\n",
      "285                                  Analytical Chemistry Scientist\n",
      "61     Data Scientist (Video Content Analysis)(KTP Associate), CSEE\n",
      "Name: job_title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# display the job titles that fit the \"data analyst\" category\n",
    "print(gdjobs_loc.loc[gdjobs_loc[\"title_dataanalyst\"],'job_title'].sample(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description of randomly selected job with title, ' Senior Data Scientist - Innovation, Advanced Analytics':\n",
      "\n",
      "Location\n",
      "12 Endeavour Square, London, E20 1JN\n",
      "Division\n",
      "Strategy & Competition\n",
      "The Role\n",
      "There are few jobs where you can make a real difference to the 40 million consumers of financial products, the 2 million people who work in the UK Financial Services industry, and the stability of our economy as a whole. The FCA has three objectives. It is responsible for:\n",
      "ensuring that markets operate with integrity;\n",
      "promoting effective competition; and\n",
      "protecting consumers of financial services.\n",
      "The RegTech & Advanced Analytics department is a newly formed function in the FCA leading the development of an organisation-wide capability to support a more analytics-led regulatory approach. Its principle objectives are delivering business value through the application of pioneering advanced analytical techniques and championing a disruptive innovation culture across the FCA. You will be at the forefront of building and shaping the Advanced Analytics centre of excellence and driving and embedding cultural change throughout the organisation.\n",
      "The department has three teams (Advanced Analytics; Transformation and RegTech) and we encourage a multi-disciplinary approach including:\n",
      "Partnering with other Divisions on analytics projects.\n",
      "Partnering with PhD programmes and commercial data science programmes to build data science capabilities and develop a pipeline of talent into the FCA.\n",
      "Working with the FCA’s Academic Advisory Council to promote the application of research to practical regulatory challenges.\n",
      "Collaborating with regulatory peers to develop best practice and share learning.\n",
      "What does the role involve? What are the key responsibilities?\n",
      "As a Technical Specialist in the Advanced Analytics team, you will use data science techniques to solve analytical problems, improve FCA operations, and advance our ability to proactively identify harm. You will deliver analytical output that can be easily understood by non-technical internal clients that meets business needs quickly and effectively. Drawing on your skills and experience you will act as a mentor for junior members of the team, to help them develop their technical capabilities and ability to solve problems quickly and economically. This will also include reviewing other team members work product to ensure that it reflects data science best practice and delivers value to the organisation.\n",
      "As part of the wider remit of the role you will promote the use of the Advanced Analytic capability across the FCA and drive thought leadership in advanced analytics in a regulatory context. This will also extend to informing and helping to develop a suite of training offerings under a new Analytical Faculty in the FCA’s Academy.\n",
      "What will the candidate get from the role?\n",
      "You will have the opportunity to be a leader in the organisation and significantly influence how the FCA builds the use of data science methods into how it operates and evaluates markets. This will help the organisation be more effective in what it does, to the benefit of consumers in the UK.\n",
      "Skills/Experience Required\n",
      "Minimum\n",
      "We’re a signatory to the Government’s Disability Confident scheme. This means that we will offer an interview to disabled candidates entering under the scheme, should they meet the minimum criteria for a role. A minimum criterion needs to be measurable from reviewing a candidate’s CV. Exceptions may apply if due to the volume of applications we are not able to interview all eligible candidates who qualify under the scheme.\n",
      "Masters degree in mathematics, statistics, computer science or related field.\n",
      "Significant proven experience in applying data science techniques and methods to practical business problems in the private or public sector.\n",
      "Excellent knowledge of data science methods including supervised (e.g. linear regression, LASSO, random forest, support vector machine) and unsupervised machine learning methods (e.g. clustering, k-nearest-neighbours).\n",
      "Advanced competency and significant applied experience using Python, R or similar to analyse, transform and visualise data.\n",
      "Essential\n",
      "Experience building, testing and deploying machine learning models into production environments.\n",
      "Experience in working with large and complex datasets using industry standard technologies (e.g. Hadoop, Spark, Hive).\n",
      "Experience working with SQL and relational databases.\n",
      "Knowledge of some/all of the following classes of methods: Natural Language Processing (e.g. HMM, LDA, Word2vec), network analysis and graph databases (e.g. Networkx, Gephi, Neo4j).\n",
      "Experience with data visualization and dashboard tools (e.g. Matplotlib, D3.js, R Shiny, Tableau, Spotfire).\n",
      "Familiarity with basic principles of distributed computing and/or distributed databases.\n",
      "Aptitude and willingness to learn and apply new tools and techniques quickly.\n",
      "Ability to deal with internal and external stakeholders through clear and professional communications both verbally and in writing.\n",
      "Ability to present highly technical information and conclusions in a way that is coherent and impactful to non-technical audiences.\n",
      "Ability to work well in a technical team where members come from diverse backgrounds and have different areas of experience and expertise.\n",
      "Focus, resilience and proven problem solving skills and behaviours. Able to continue to find solutions and work towards goals when setbacks/issues occur.\n",
      "Desire to remain at forefront of data science research and understand where we can apply leading edge techniques to help solve complex problems.\n",
      "Ability to maintain a network of external expert advisers, from academia and beyond.\n",
      "Desire to deliver practical insights to help the way we regulate.\n",
      "Desirable\n",
      "Experience applying data science techniques in the Financial Services industry.\n",
      "Experience in financial regulation or other regulatory areas.\n",
      "Programming experience in C/Java/Scala.\n",
      "Experience with reinforcement and/or deep learning.\n",
      "Application of GPU computing platforms/architecture (e.g. CUDA)\n",
      "About the FCA\n",
      "At the FCA, we’re creating a fair and more resilient financial system. We’re establishing more transparent relationships between financial services and their customers, building trust in financial markets and protecting vulnerable consumers.\n",
      "The FCA's Values & Diversity\n",
      "Our ambition is to create a diverse and inclusive workplace that reflects the society we serve, helping us to be a better regulator.\n",
      "\n",
      "As an inclusive employer, we are open to considering flexible working arrangements. Please contact our recruiter if you wish to apply for this role on a flexible basis.\n",
      "Useful Information\n",
      "This role is graded Technical Specialist - REG D1\n",
      "Salary £70,000 - £85,000\n",
      "Applications for this role close on Wednesday 23rd December 2020\n",
      "If you are interested learning more about the role:\n",
      "For internal applications, contact Daniel Dowling on daniel.dowling@fca.org.uk\n",
      "For external applications, contact Ciara McCartan on Ciara.McCartan@fca.org.uk\n",
      "Please note – Applications must be submitted through our online portal. Applications sent via email will not be accepted.\n"
     ]
    }
   ],
   "source": [
    "# scan job descriptions; keep running this cell until you've reviewed enough samples\n",
    "sample_description(jobsdf=gdjobs_loc, col=\"title_dataanalyst\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having scanned the job descriptions of the data analyst roles that were returned in this search of \"data scientist\" jobs on glass door, there is considerable overlap with the \"data scientist\" roles. However, they don't seem to mention machine learning as often as 'data scientist' roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 (8%) job titles indicate a 'data engineer' role\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a regular expression to identfy \"data engineer\" job titles and \n",
    "# create a boolean column to record whether or not each job title fits this category\n",
    "gdjobs_loc[\"title_dataengineer\"] = gdjobs_loc[\"job_title\"].str.contains(\n",
    "    r\"data engineer|devops\", regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "# how many job titles indicate \"data engineer\" role? (\"True\" count)\n",
    "print(\n",
    "    f\"{sum(gdjobs_loc.title_dataengineer)} ({round((np.mean(gdjobs_loc.title_dataengineer))*100)}%)\",\n",
    "    \"job titles indicate a 'data engineer' role\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288                       Data Engineer\n",
      "284                       Data Engineer\n",
      "365                       Data Engineer\n",
      "478        Data Engineer - Test Analyst\n",
      "393    Data Engineer (Machine Learning)\n",
      "398                       DATA ENGINEER\n",
      "183                 Cloud Data Engineer\n",
      "364                       Data Engineer\n",
      "333                       Data Engineer\n",
      "499                       Data Engineer\n",
      "276                       Data Engineer\n",
      "418     Big Data Engineer (Python Team)\n",
      "270                       Data Engineer\n",
      "436        Data Engineer (12 month FTC)\n",
      "498                       Data Engineer\n",
      "388                Senior Data Engineer\n",
      "287                       Data Engineer\n",
      "339                       Data Engineer\n",
      "440                       Data Engineer\n",
      "275                       Data Engineer\n",
      "Name: job_title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# display the job titles that fit the \"data engineer\" category\n",
    "print(gdjobs_loc.loc[gdjobs_loc[\"title_dataengineer\"],'job_title'].sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description of randomly selected job with title, ' Data Engineer':\n",
      "\n",
      "DATA ENGINEER\n",
      "READING (REMOTE WORKING)\n",
      "UP TO £60,000\n",
      "Harnham are partnered with a global FMCG company who are looking for a data engineer to join their R&D team to help productionise their ML models.\n",
      "THE COMPANY\n",
      "This company are a global leader in selling consumer products. They have offices across the world that work closely with each other to produce consistent results. This company ingests huge volumes of data due to their size and require a data engineer to help build pipelines to ingest, transform and distribute data. Their UK R&D team is based in Reading and are working with Computer Vision to detect faults with products on their supply chains.\n",
      "THE ROLE\n",
      "You will sit within the R&D team, working with cutting-edge technology on Data Science specific projects.\n",
      "You will be building data pipelines to ingest, transform and distribute data according to R&D initiatives\n",
      "You will be deploying ML models and solutions development with data scientists.\n",
      "You will be developing solutions and working through it in an end-to-end role: from modelling, through to implementation.\n",
      "YOUR SKILLS AND EXPERIENCES\n",
      "You will have extensive experience in Python.\n",
      "You will have experience in cloud technologies such as AWS, GCP or Azure.\n",
      "You will have worked with SQL and No-SQL databases.\n",
      "You will need an Masters or a PhD in a stats based subject.\n",
      "HOW TO APPLY\n",
      "Please register your interest by sending you CV to Callum Castling via the apply link on this page.\n"
     ]
    }
   ],
   "source": [
    "# scan job descriptions; keep running this cell until you've reviewed enough samples\n",
    "sample_description(jobsdf=gdjobs_loc, col=\"title_dataengineer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scanning the description of the \"Data Engineer\" jobs indicates that this role is distinct from a data scientist job; data engineers know how to build an effective data architecture, streamline data processing, and maintain large-scale data systems. In addition to working with Python or R, they likely also work with other languages to create data engineering pipelines, automate common file system tasks, and build high-performance databases. They also need to know how to use cloud and big data tools such as AWS Boto, PySpark, Spark SQL, and MongoDB, to create and query databases, wrangle data, and configure schedules to run pipelines. They need database, scripting, and process skills. We will exclude these positions from our analysis of data science jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Machine Learning/AI Specialist or Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 (6%) job titles indicate a 'machine learning/AI specialist/engineer' role\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a regular expression to identfy \"machine learning/AI specialist/engineer\" job titles and \n",
    "# create a boolean column to record whether or not each job title fits this category\n",
    "gdjobs_loc[\"title_mlai\"] = gdjobs_loc[\"job_title\"].str.contains(\n",
    "    r\"machine learning engineer|ml engineer|machine learning scientist|machine learning|\\bai\\b|artificial intelligence\", \n",
    "    regex=True, \n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# how many job titles indicate \"machine learning/AI specialist/engineer\" role? (\"True\" count)\n",
    "print(\n",
    "    f\"{sum(gdjobs_loc.title_mlai)} ({round((np.mean(gdjobs_loc.title_mlai))*100)}%)\",\n",
    "    \"job titles indicate a 'machine learning/AI specialist/engineer' role\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412                                               Data and ML Engineer\n",
      "40                                  Applied Machine Learning Scientist\n",
      "403                                Machine Learning Scientist (London)\n",
      "268                  Senior Data Scientist/ Machine Learning Developer\n",
      "254                                      Visiting Scientist, AI (EMEA)\n",
      "368               Applied Scientist in Machine Learning for Simulation\n",
      "481                         AI Scientist - Natural Language Processing\n",
      "7       Senior Data Scientist- (Machine Learning & Advanced Analytics)\n",
      "225    Outside IR35 | Data Scientist | AI | Python | Contract | London\n",
      "228                     Data Scientist - Machine Learning (Python/SQL)\n",
      "419                           Data Science - Machine Learning Research\n",
      "515                     Principal Research Scientist: Machine Learning\n",
      "328                 Scientific Data Analyst - Machine Learning, Python\n",
      "39                                   Data Scientist - Machine Learning\n",
      "618                           Artificial Intelligence – Data Scientist\n",
      "396                                             Visiting Scientist, AI\n",
      "688                                  Data Scientist – Machine Learning\n",
      "257          Applied Machine Learning Scientist (Remote, London-Based)\n",
      "361              Data Scientist - Innovative Applications of Causal AI\n",
      "94                                    Data Engineer (Machine Learning)\n",
      "Name: job_title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# display the job titles that fit the \"machine learning/AI specialist/engineer\" category\n",
    "print(gdjobs_loc.loc[gdjobs_loc[\"title_mlai\"], 'job_title'].sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description of randomly selected job with title, ' Senior AI Data Scientist/Engineer':\n",
      "\n",
      "Senior AI Data Scientist/Engineer London (current remote)\n",
      "Permanent\n",
      "Up to £80,000 perm annum\n",
      "\n",
      "I am working with a leading data consultancy who have recently gone through a huge period of growth allowing them to invest in their AI and Data Science practices.\n",
      "\n",
      "You will have the chance to work alongside a a team of talented software engineers as well as developing team of data science and AI professionals. You will be able to get stuck into some really exciting projects with a focus on those core data science principles.\n",
      "\n",
      "As a Senior in the team you will have a say in projects planning and work closely with the technical architects and lead to successfully deliver projects across a range of technologies in your space. You will ideally be a seasoned data science professional who is ready to take that next step in their career.\n",
      "\n",
      "Skills:\n",
      "Proficient in designing, building, testing and maintaining production software applications that integrate AI services and include data science models.\n",
      "Experience with major machine learning frameworks, languages and NLP frameworks.\n",
      "Proficient applying mathematics principles to gain insight from data.\n",
      "Experience of design and development across multiple layers of an application (Python and R)\n",
      "Implemented and developed new machine learning approaches and services.\n",
      "Knowledge of cloud platforms (AWS, GCP, AZURE)\n",
      "It is important that you are passionate about your work as the projects you will be working on will have a real life impact.\n",
      "\n",
      "If you are interested and want to hear more about the opportunity drop me an email to sam.jeffreys@opusrs.com or apply below\n"
     ]
    }
   ],
   "source": [
    "# scan job descriptions; keep running this cell until you've reviewed enough samples\n",
    "sample_description(jobsdf=gdjobs_loc, col=\"title_mlai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions of jobs with titles that include 'machine learning' and 'artificial intelligence', sometimes with term 'engineer', tend to involve researching, designing, building, testing and optimizing machine learning/AI algorithms/models and systems that can learn and be used to make predictions. However, since our glassdoor search was for UK 'data scientist' jobs, the jobs that fall into this category almost always overlap with the general data scientist roles that don't mention ML/AI in the title, many of which still mention machine learning knowledge and skills. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Science or Machine Learning Research Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 (3%) job titles indicate a 'data science or ML research scientist' role\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a regular expression to identfy \"data science or ML research scientist\" job titles \n",
    "# and create a boolean column to record whether or not each job title fits this category\n",
    "gdjobs_loc[\"title_research\"] = gdjobs_loc[\"job_title\"].str.contains(\n",
    "    r\"research\", regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "# how many job titles indicate \"data science or ML research scientist\" role? (\"True\" count)\n",
    "print(\n",
    "    f\"{sum(gdjobs_loc.title_research)} ({round((np.mean(gdjobs_loc.title_research))*100)}%)\",\n",
    "    \"job titles indicate a 'data science or ML research scientist' role\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73                                                                  Research Officer and Data Scientist\n",
      "493                                                                         Engineer Research Scientist\n",
      "41                                                              Data Scientist / Operational Researcher\n",
      "410                                                      Senior Data Scientist / Operational Researcher\n",
      "726                                                                    Researcher/Data Scientist - QMUL\n",
      "447    Research Associate III/Senior Research Associate Data Collection Peri- and Post Approval Studies\n",
      "465                                                            Research Scientist Video Compression R&D\n",
      "25                                                                Research Scientist Evidence Synthesis\n",
      "419                                                            Data Science - Machine Learning Research\n",
      "477                                         Imaging Research Scientist (Machine Learning/Deep Learning)\n",
      "642                                                                   Data Scientist - Applied Research\n",
      "695                                                             Data Scientist / Operational Researcher\n",
      "581                                                               Research Scientist – Causality Expert\n",
      "509                                                             919-LO-33499143-EXT -Research Scientist\n",
      "483                                            Research Scientist Peri-and-Post Approval Studies (PPAS)\n",
      "168                                                                             Research Data Scientist\n",
      "22                                                        Senior Research Associate/Associate Scientist\n",
      "522                                           Machine Learning Researcher (NLP, Python, Data Scientist)\n",
      "505                                                              Research Scientist - Protein Scientist\n",
      "583                                                           Quantitative Research Director - Big Data\n",
      "Name: job_title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# display the job titles that fit the \"data science or ML research scientist\" category\n",
    "print(gdjobs_loc.loc[gdjobs_loc[\"title_research\"],\"job_title\"].sample(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description of randomly selected job with title, ' Research Associate III/Senior Research Associate Data Collection Peri- and Post Approval Studies':\n",
      "\n",
      "Research Associate III (RAIII)/Senior Research Associate (SRA)- Data Collection– Peri- and Post Approval Studies\n",
      "*We are looking to fill this role in our London, UK office; we will consider other locations based on the candidates’ experience and qualifications\n",
      "The Team\n",
      "Evidera has been providing epidemiology, data analytics, and outcomes research services to clients in the biopharmaceutical industry for over 19 years. The Peri- and Post Approval Studies team supports pharmaceutical/ biotechnology/ medical device companies in the design and conduct of real-world observational studies throughout the product lifecycle, from early pre-launch planning to launch and post-marketing management. Our focus is on helping our clients identify evidence gaps and rapidly build epidemiologic and economic evidence to demonstrate the effectiveness, safety and value of their products. For this role, the focus is on de novo (field) data collection studies such as single and multi-country retrospective chart reviews, surveys, prospective studies, registries and post-market safety studies which are undertaken when existing health databases cannot be used. Increasingly, hybrid studies that invoke the use of both secondary data sources with bespoke data collection methods are deployed to address our clients’ needs.\n",
      "\n",
      "Data collected in these studies is then used to address key research questions such as:\n",
      "Burden of disease and identification of unmet needs\n",
      "Treatment patterns, drug utilization, adherence\n",
      "Resource utilization\n",
      "The identification and impact of risk factors on disease outcomes\n",
      "Comparative effectiveness\n",
      "Safety outcomes\n",
      "We use epidemiologic methods to build knowledge of the relationships between patient, drug, clinical and disease factors that significantly aids our clients and the patients they serve in understanding health conditions and navigating potential drug-event causal relationships as well as in building stronger evidence-based value messages.\n",
      "Position Overview\n",
      "The Research Associate III/SRA in Data Collection drafts project deliverables, interacts with clients, and contributes to strategic thinking under the direction of senior staff. Supports scientific coordination and project management, including scoping, tracking progress, and coordinating team tasks for multiple projects.\n",
      "Principal Duties and Responsibilities (Essential Functions**):\n",
      "The Research Associate III/SRA in Data Collection will bear scientific responsibility for the design and implementation of de novo data collection studies (retrospective, prospective, and/or cross-sectional) under oversight of more senior staff.\n",
      "\n",
      "In particular, the Research Associate III/ SRA will:\n",
      "Provide scientific guidance/consultation on scientific methodological and operational considerations of study design and conduct\n",
      "Lead on or oversee the development of draft scientific project deliverables (e.g., protocols, analysis plans, case report forms (electronic or paper), survey questions, reports) for senior review.\n",
      "Interact directly with study sponsors, physician experts, and clinical sites\n",
      "Assist the responsible scientist with monitoring project budget spend and managing timelines for scientific tasks/deliverables.\n",
      "Contributes to abstracts and manuscripts as co-author.\n",
      "Contribute to the growth of Evidera through business development activities, including support of proposal development, participation in the sales cycle, conference attendance, and presentations\n",
      "Support other organizational activities as needed (e.g. supervise and/or mentor junior staff; closely collaborate with other project stakeholders such as clinical operations, data management, biostatistics, and data analysts; contribute to development of training materials and process improvements)\n",
      "Travel (national and international travel) is expected\n",
      "Consulting Expectations:\n",
      "Participates in client calls and is expected to be able to address questions posed directly to him/her about his/her work, otherwise participates minimally.\n",
      "Contributes to strategic thinking under the direction of senior staff.\n",
      "Managerial Duties: None\n",
      "Education, Professional Skills & Experience\n",
      "PhD (epidemiology/pharmacoepidemiology or closely related field) or MSc with relevant experience\n",
      "Experience with de novo data collection (field) studies (non-interventional designs in particular)\n",
      "Good understanding of epidemiologic study designs\n",
      "Previous experience in consultancy and/or CRO environment highly desirable\n",
      "Working knowledge of MS Office software; working knowledge of data analysis software (e.g., SAS, Stata, R) a plus\n",
      "Demonstrated ability to communicate and document information effectively in written English, including the ability to produce scientific reports, presentations, emails and other written deliverables that are well-organized, free from errors, and are appropriately adjusted to the characteristics and needs of the audience.\n",
      "Strong organizational, time-management, prioritization and decision-making skills necessary to evaluate, plan and accomplish multiple work goals in a timely fashion.\n",
      "Able to quickly learn and apply new information, skills and procedures.\n",
      "Proactive and positive approach to tasks and projects, as well as resilience when encountering process, project, budget or client changes that are inherent in the consulting environment.\n",
      "Strong communication and interpersonal skills including the ability to quickly develop and sustain positive rapport with internal and external contacts in person, over the phone and by email.\n",
      "Experience presenting ideas to individuals and groups in a formal presentation setting\n",
      "Delivers a positive internal/external client experience; listens and understand the client's needs, and recommends solutions\n",
      "Demonstrates sound professional judgment in analysing, responding to, and resolving enquiries, issues and escalations.\n",
      "Experience with design of peri- or post-approval late phase interventional studies is not required but would be considered a plus\n",
      "About Evidera:\n",
      "Evidera is a business within in Pharmaceutical Product Development, LLC (PPD) a leading global contract research organization (CRO), and a is a preeminent provider of evidence-based solutions. We provide integrated scientific expertise and global operational capabilities to help clients generate the evidence needed to optimize the market access and commercial potential of products.\n",
      "\n",
      "Perks:\n",
      "We offer a competitive salary and benefits package, with clear opportunities for growth and career progression. You will have the opportunity to work on multiple projects with some of the industry’s leading researchers. Our offices boast a fun and collaborative working environment, frequent social events and a robust support system. We are committed to providing training and professional development, with ample opportunity to advance, for all our staff.\n",
      "\n",
      "Evidera’s Core Competencies:\n",
      "Customer Focus\n",
      "Initiative\n",
      "Teamwork\n",
      "Problem Solving/Judgment\n",
      "Accountability\n",
      "If you resonate with our core competencies and want to contribute to research and consulting services driven by world-class science and thought leadership, then please submit your application – we’d love to hear from you.\n",
      "Evidera, Inc. provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, genetics, sexual orientation, gender preference,disability, or status as a qualified individual with a disability or protected veteran.\n"
     ]
    }
   ],
   "source": [
    "# scan job descriptions; keep running this cell until you've reviewed enough samples\n",
    "sample_description(jobsdf=gdjobs_loc, col=\"title_research\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jobs that include 'research' in the title tend to be more specialised and technical, often require the candidate to have a PhD, and involve researching and coming up with novel solutions/algorithms to address difficult machine learning/deep learning/AI problems. Within this category are jobs that require indepth knowledge/experience within a particular domain, e.g. operations research, biomedical data. Some are University positions, others are within interdisciplinary data science teams within businesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Science Intern/Apprentice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 (2%) job titles indicate a 'data science intern/apprentice' role\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a regular expression to identfy \"data science intern/apprentice\" job titles and create a boolean column to record whether or not each job title fits this category\n",
    "gdjobs_loc[\"title_internapprentice\"] = gdjobs_loc[\"job_title\"].str.contains(\n",
    "    r\"\\binternship|\\bintern\\b|\\bapprentic\", regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "# how many job titles indicate \"data science intern/apprentice\" role? (\"True\" count)\n",
    "print(\n",
    "    f\"{sum(gdjobs_loc.title_internapprentice)} ({round((np.mean(gdjobs_loc.title_internapprentice))*100)}%)\",\n",
    "    f\"job titles indicate a 'data science intern/apprentice' role\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43                                              Internship- Data Science\n",
      "159           Data Scientist Degree Apprenticeship, Barnard Castle, 2021\n",
      "179                Data Scientist Degree Apprenticeship, Stevenage, 2021\n",
      "188    Data Scientist Degree Apprenticeship, GSK House (Brentford), 2021\n",
      "190                     Data Scientist Degree Apprenticeship, Ware, 2021\n",
      "Name: job_title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# display the job titles that fit the \"data science intern/apprentice\" category\n",
    "print(gdjobs_loc.loc[gdjobs_loc[\"title_internapprentice\"],'job_title'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description of randomly selected job with title, ' Data Scientist Degree Apprenticeship, GSK House (Brentford), 2021':\n",
      "\n",
      "Site Name: UK - London - Brentford\n",
      "Posted Date: Nov 25 2020\n",
      "\n",
      "Exciting minds with our Data Science (R&D) Apprenticeship, Brentford\n",
      "\n",
      "Education required:\n",
      "5 GCSEs or equivalent grade 9-5 (A*-C) including Maths and English Language (not Literature).\n",
      "Must have/be predicted 96 UCAS points from your top 3 A levels, each at grade C and above.\n",
      "Must have A level Maths at grade C (or above) alternatively A level Computer Science at grade B (or above) plus Maths GCSE at grade 7-9\n",
      "Start date: September 2021\n",
      "\n",
      "Assessment centre dates: March 2021\n",
      "\n",
      "We accept ongoing applications and will close this vacancy once we have enough applications, so its best to apply as soon as possible as we do not want you to miss out!\n",
      "\n",
      "We want you to be motivated and passionate for the Apprenticeship you apply to we will only accept ONE Apprenticeship application per candidate each year. Please do your research before applying and select the one Apprenticeship you wish to apply to good luck with your application!\n",
      "\n",
      "Want to solve some of the worlds biggest health challenges?\n",
      "\n",
      "With over 300 years of innovation, were a science-led global healthcare company that delivers billions of innovative products each year. Join us and help millions of people do more, feel better, live longer.\n",
      "\n",
      "What will you do?\n",
      "\n",
      "Data Science is a broad and fast-moving field spanning maths and statistics, software engineering and communications. Data Scientists blend experience and knowledge from a wide range of fields and organisations, and continuously seek to expand their range of technical skills.\n",
      "\n",
      "Our Data Science apprentices, based at GSK House in West London, support drug discovery and development through the integrative analysis of internal and/or external biomedical data. On our 4-year programme youll receive tailored on-the-job training from industry experts from day one combined with early responsibility to help you to develop and progress. Youll be working towards a Bachelors degree (BSc) in Data Science.\n",
      "\n",
      "At GSK our data analysis competencies are quite diverse and can include: genetics and genomics analyses (e.g. gene expression, integrative omics, functional genomics), pathways and networks analyses, disease indications findings, molecular evolution.\n",
      "\n",
      "You will be:\n",
      "Analysing biomedical data, especially genomics data, to support drug discovery with a focus on target identification and validation in immunology and oncology\n",
      "Developing and delivering support and guidance materials for new and non-expert users to access and use deployed genomics and bioinformatics data and solutions\n",
      "Contribute to AI and machine learning projects that drive insight into multiple phases of the drug development pipeline from target identification and validation to later phases\n",
      "Working in an area that results in impacting development of new medicines for important diseases of unmet medical need\n",
      "Working on a variety of data analysis competencies and building your scientific knowledge, on a tailored development programme\n",
      "Learning how to prioritise and manage multiple projects, objectives and deadlines with high attention to detail\n",
      "Learning new data skills and concepts\n",
      "What you do day-to-day will depend on the team that you join. Watch the video above to hear from our current Data Science apprentices.\n",
      "\n",
      "Achieve your career goals on our apprentice programme\n",
      "\n",
      "If you are offered a role; you will have a buddy or a mentor who will help you to prepare for joining GSK and will be there to offer support and guidance throughout your apprenticeship. Working with both your Line and Programme Manager we want to identify the right opportunities to help you succeed in the short term and throughout your career with us.\n",
      "\n",
      "Benefits include:\n",
      "A competitive base salary of £17,000\n",
      "An annual bonus\n",
      "Benefit programmes benchmarked against industry standards\n",
      "A recognition programme to reward exceptional work by you and your team\n",
      "Training and Development (including opportunities to attend annual Apprentice development events)\n",
      "Access to employee health and well-being programmes\n",
      "A fully-sponsored Bachelors degree qualification\n",
      "Get to grips with our recruitment process\n",
      "\n",
      "Youll find hints, tips and guidance on our recruitment process on our website - https://www.gsk.com/en-gb/careers/apprentices-students-and-graduates/apprentice-programmes/applying-for-our-apprentice-programme/\n",
      "\n",
      "We have changed the way we recruit in light of the COVID-19 pandemic, you can learn more about how our practices have adapted here.\n",
      "\n",
      "Apply Now\n",
      "\n",
      "We accept ongoing applications and will close this vacancy once we have enough applications, so its best to apply as soon as possible as we do not want you to miss out!\n",
      "\n",
      "#UKApprenticeships\n",
      "\n",
      "#DataScienceApprenticeships\n",
      "\n",
      "Were 100% open to all talent\n",
      "\n",
      "Were 100% open to all talent whatever your gender, marital status, religion, age, colour, race, sexual orientation, nationality or disability. We want to recruit the right people for GSK from the widest possible backgrounds, so we can better serve the diversity of our patients and consumers. And also because its the right thing to do.\n",
      "\n",
      "You can learn more about Inclusion and diversity at GSK here.\n",
      "\n",
      "Need help with your application?\n",
      "\n",
      "Please email us at uk.earlytalentrecruitment@gsk.com and let us know how we can help you. Or email us your number and well call you back. Well make all reasonable modifications to support you throughout the recruitment process and well treat all information you give us in confidence. Please get in touch to help us process your application as soon as possible.\n",
      "\n",
      "As a company driven by our values of Patient focus, Transparency, Respect and Integrity, we know inclusion and diversity are essential for us to be able to succeed. We want all our colleagues to thrive at GSK bringing their unique experiences, ensuring they feel good and to keep growing their careers. As a candidate for a role, we want you to feel the same way.\n",
      "\n",
      "As an Equal Opportunity Employer, we are open to all talent. In the US, we also adhere to Affirmative Action principles. This ensures that all qualified applicants will receive equal consideration for employment without regard to race/ethnicity, colour, national origin, religion, gender, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class*(*US only).\n",
      "\n",
      "We believe in an agile working culture for all our roles. If flexibility is important to you, we encourage you to explore with our hiring team what the opportunities are.\n",
      "\n",
      "Please dont hesitate to contact us if youd like to discuss any adjustments to our process which might help you demonstrate your strengths and capabilities. You can either call us on 0808 234 4391, or send an email ukdiversity.recruitment@gsk.com\n",
      "\n",
      "As you apply, we will ask you to share some personal information which is entirely voluntary. We want to have an opportunity to consider a diverse pool of qualified candidates and this information will assist us in meeting that objective and in understanding how well we are doing against our inclusion and diversity ambitions. We would really appreciate it if you could take a few moments to complete it. Rest assured, Hiring Managers do not have access to this information and we will treat your information confidentially.\n",
      "\n",
      "Important notice to Employment businesses/ Agencies\n",
      "\n",
      "GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.\n",
      "\n",
      "Please note that if you are a US Licensed Healthcare Professional or Healthcare Professional as defined by the laws of the state issuing your license, GSK may be required to capture and report expenses GSK incurs, on your behalf, in the event you are afforded an interview for employment. This capture of applicable transfers of value is necessary to ensure GSKs compliance to all federal and state US Transparency requirements. For more information, please visit GSKs Transparency Reporting For the Record site.\n"
     ]
    }
   ],
   "source": [
    "# scan job descriptions; keep running this cell until you've reviewed enough samples\n",
    "sample_description(jobsdf=gdjobs_loc, col=\"title_internapprentice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data science internships and apprenticeships that come up in our search for data science jobs fall into 3 types of roles:\n",
    "- essentially a full data scientist role for new graduates, likely so the company can trial the graduate before hiring\n",
    "- mid-degree industry placement roles\n",
    "- apprenticeships for people leaving school with science a-levels (including a company sponsored data science degree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having scanned many of the job descriptions in the GlassDoor \"data scientist\" jobs data set, I believe the Data Scientist role to be best represented by the job titles falling into the following categories *only*:\n",
    "\n",
    "- Data Scientist\n",
    "- Data Analyst\n",
    "- Machine Learning/AI Specialist or Engineer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not those that fall under the following:\n",
    "\n",
    "- Data Engineer\n",
    "- Researcher\n",
    "- Intern/Apprentice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511 (71%) job titles indicate a data scientist role (data scientists, analysts and ML/AI specialists/engineers)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a boolean mask for jobs that have titles that fall into the following categories: \n",
    "# \"data scientist\", \"data analyst\", \"machine learning/ai specialist or engineer\"\n",
    "gdjobs_loc[\"datascience_role\"] = (\n",
    "    ((gdjobs_loc[\"title_datascientist\"] == True) | (gdjobs_loc[\"title_mlai\"] == True) | (gdjobs_loc[\"title_dataanalyst\"] == True))\n",
    ") & (\n",
    "    (gdjobs_loc[\"title_internapprentice\"] == False) & (gdjobs_loc[\"title_dataengineer\"] == False) & (gdjobs_loc[\"title_research\"] == False)\n",
    ")\n",
    "\n",
    "# how many job titles indicate a data scientist role? (\"True\" count)\n",
    "print(\n",
    "    f\"{sum(gdjobs_loc.datascience_role)} ({round((np.mean(gdjobs_loc.datascience_role))*100)}%)\",\n",
    "    f\"job titles indicate a data scientist role (data scientists, analysts and ML/AI specialists/engineers)\\n\"\n",
    ")\n",
    "\n",
    "# create a dataframe of data science roles only\n",
    "dsjobs = gdjobs_loc[gdjobs_loc[\"datascience_role\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 511 entries, 0 to 725\n",
      "Data columns (total 33 columns):\n",
      " #   Column                  Non-Null Count  Dtype   \n",
      "---  ------                  --------------  -----   \n",
      " 0   job_title               511 non-null    object  \n",
      " 1   salary_estimate         322 non-null    object  \n",
      " 2   job_description         511 non-null    object  \n",
      " 3   rating                  399 non-null    float64 \n",
      " 4   company_name            511 non-null    object  \n",
      " 5   location                511 non-null    object  \n",
      " 6   size                    415 non-null    category\n",
      " 7   founded                 348 non-null    Int64   \n",
      " 8   type_of_ownership       428 non-null    category\n",
      " 9   industry                381 non-null    category\n",
      " 10  sector                  383 non-null    category\n",
      " 11  revenue                 271 non-null    category\n",
      " 12  rating_culturevalues    389 non-null    float64 \n",
      " 13  rating_worklifebalance  394 non-null    float64 \n",
      " 14  rating_diversity        296 non-null    float64 \n",
      " 15  rating_seniormgmt       394 non-null    float64 \n",
      " 16  rating_compbenefits     394 non-null    float64 \n",
      " 17  rating_careerops        394 non-null    float64 \n",
      " 18  api_citytownvilham      488 non-null    object  \n",
      " 19  api_region              488 non-null    category\n",
      " 20  api_country             491 non-null    category\n",
      " 21  uk                      511 non-null    bool    \n",
      " 22  remote                  511 non-null    bool    \n",
      " 23  salary_min              322 non-null    object  \n",
      " 24  salary_max              322 non-null    object  \n",
      " 25  salary_mid              322 non-null    float64 \n",
      " 26  title_datascientist     511 non-null    bool    \n",
      " 27  title_dataanalyst       511 non-null    bool    \n",
      " 28  title_dataengineer      511 non-null    bool    \n",
      " 29  title_mlai              511 non-null    bool    \n",
      " 30  title_research          511 non-null    bool    \n",
      " 31  title_internapprentice  511 non-null    bool    \n",
      " 32  datascience_role        511 non-null    bool    \n",
      "dtypes: Int64(1), bool(9), category(7), float64(8), object(8)\n",
      "memory usage: 85.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dsjobs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data have been cleaned, they are ready for feature engineering and analysing! Since some jobs were removed from the data set during the cleaning process, the DataFrame needs to be reindexed before it is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index; 'drop' stops old index being inserted as a column\n",
    "dsjobs.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned dataframe \n",
    "# as a .csv file\n",
    "dsjobs.to_csv(os.path.join(path, f'dsjobs_df_{scrapedate}_postclean.csv'), encoding='utf-8')\n",
    "# as a .pkl file which preserves data types (better for processing steps)\n",
    "dsjobs.to_pickle(os.path.join(path, f'dsjobs_df_{scrapedate}_postclean.pkl'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('eda-proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "ecf77a1a389081cd4a2e68fe826e74ff3c59ede00e7da793e7feb1b74b90842f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
